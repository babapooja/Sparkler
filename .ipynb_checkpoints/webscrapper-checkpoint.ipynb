{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f64cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, json\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\"\n",
    "}\n",
    "params = {\n",
    "    \"q\": \"\",\n",
    "    \"hl\": \"en\",\n",
    "    \"start\": 0\n",
    "}\n",
    "from tkinter import *\n",
    "window=Tk()\n",
    "\n",
    "link = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5feb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar Articles\n",
    "def getSimilarArticles(q,df, vectorizer):    \n",
    "    q = [q]\n",
    "    q_vector = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "    print(q_vector)\n",
    "    sim = {}\n",
    "    \n",
    "    for i in range(df.shape[1]):\n",
    "        sim[i] = np.dot(df.loc[:, i].values, q_vector) / np.linalg.norm(df.loc[:,i]) * np.linalg.norm(q_vector)\n",
    "        \n",
    "    sim_sorted = sorted(sim.items(), key = lambda x:x[1], reverse = True)\n",
    "    print(sim_sorted)\n",
    "    for k,v in sim_sorted[:5]:\n",
    "        if v != 0.0:\n",
    "            print(\"Similarities: {}\".format(v))\n",
    "            print('Link to the article:', link[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8673d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizerMethod(documents_clean):\n",
    "    # Instantiate a TfIdfVectorizer Object and transform the data to vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(documents_clean)\n",
    "    X = X.T.toarray()\n",
    "\n",
    "    df = pd.DataFrame(X, index=vectorizer.get_feature_names_out())\n",
    "\n",
    "#     pprint.pprint(df.filter(like='bioinformatics', axis=0))\n",
    "#     print(df.loc[:, 0])\n",
    "    getSimilarArticles(params['q'], df, vectorizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9a0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(documents):\n",
    "    documents_clean = []\n",
    "    for d in documents:\n",
    "        # Remove Unicode\n",
    "        document_test = re.sub(r'[^\\x00-\\x7F]+', ' ', d)\n",
    "        # Remove Mentions\n",
    "        document_test = re.sub(r'@\\w+', '', document_test)\n",
    "        # Lowercase the document\n",
    "        document_test = document_test.lower()\n",
    "        # Remove punctuations\n",
    "        document_test = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', document_test)\n",
    "        # Lowercase the numbers\n",
    "        document_test = re.sub(r'[0-9]', '', document_test)\n",
    "        # Remove the doubled space\n",
    "        document_test = re.sub(r'\\s{2,}', ' ', document_test)\n",
    "        documents_clean.append(document_test)\n",
    "    vectorizerMethod(documents_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c6c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectDocumentFromLinks(link):\n",
    "    #Retrieve paragrahs from each link, combine each paragrah as a string and save it to docs\n",
    "    documents = []\n",
    "    print('Started...')\n",
    "    for i in link:\n",
    "        r = requests.get(i, headers=headers)\n",
    "        soup = BeautifulSoup(r.content,'html.parser')\n",
    "\n",
    "        sen = []\n",
    "        # for springer abstracts\n",
    "        if soup.find('div', {'id':'Abs1-content'}):\n",
    "            for i in soup.find('div', {'id':'Abs1-content'}).find_all('p'):\n",
    "                sen.append(i.text)\n",
    "\n",
    "    #     if soup.find('div', {'class': 'article-section__content en main'}):\n",
    "    #         for i in soup.find('div', {'class': 'article-section__content en main'}).find_all('p'):\n",
    "    #             sen.append(i.text)\n",
    "\n",
    "        documents.append(' '.join(sen))\n",
    "    print('Completed...')\n",
    "    cleanData(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a99bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectSpringerLinks(soup):\n",
    "    #Retrieve all popular new links\n",
    "    print(len(soup))\n",
    "    i=0\n",
    "    for i in range(0, len(soup)):\n",
    "        data = soup[i].find_all(\"div\", {\"class\": \"gs_ri\"})\n",
    "        for j in range(len(data)):    \n",
    "\n",
    "            temp = data[j].find('a')\n",
    "\n",
    "            if 'link.springer.com/article' in temp['href'] and 'books.google.com' not in temp['href']:\n",
    "                link.append(temp['href'])\n",
    "    #             print(temp['href'], j)\n",
    "\n",
    "\n",
    "    collectDocumentFromLinks(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed094191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectGoogleScholarPages(query):\n",
    "    params['q'] = query\n",
    "    soup = []\n",
    "    while True:\n",
    "        url = 'https://scholar.google.com/scholar'\n",
    "        req = requests.get(url, headers=headers, params=params)\n",
    "        print(req.url)\n",
    "        tempData = BeautifulSoup(req.content,'html.parser')\n",
    "        soup.append(tempData)\n",
    "        if tempData.find('span', {'class': 'gs_ico gs_ico_nav_next'}) and params['start']<=100:\n",
    "            params['start']+=10\n",
    "        else:\n",
    "            break\n",
    "    # call collectSpringerLinks()\n",
    "    collectSpringerLinks(soup)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37613a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/sorry/index?continue=https://scholar.google.com/scholar%3Fq%3Diot%252C%2Bspringer%26hl%3Den%26start%3D0&hl=en&q=EgSDYN94GPGF2psGIiykjrXSUPlHMm9fEQBEMYHAgJilBZupWIM4o7TODI9SA0NatttPOqMv9JpRtzIBcg\n",
      "1\n",
      "Started...\n",
      "Completed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\babap\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\babap\\AppData\\Local\\Temp\\ipykernel_21520\\337224248.py\", line 8, in <lambda>\n",
      "    btn=Button(window, text=\"Search Documents\", fg='black', command=lambda: collectGoogleScholarPages(txtfld.get()))\n",
      "  File \"C:\\Users\\babap\\AppData\\Local\\Temp\\ipykernel_21520\\4243393285.py\", line 15, in collectGoogleScholarPages\n",
      "    collectSpringerLinks(soup)\n",
      "  File \"C:\\Users\\babap\\AppData\\Local\\Temp\\ipykernel_21520\\1108676299.py\", line 16, in collectSpringerLinks\n",
      "    collectDocumentFromLinks(link)\n",
      "  File \"C:\\Users\\babap\\AppData\\Local\\Temp\\ipykernel_21520\\3788052889.py\", line 21, in collectDocumentFromLinks\n",
      "    cleanData(documents)\n",
      "  File \"C:\\Users\\babap\\AppData\\Local\\Temp\\ipykernel_21520\\1261891663.py\", line 17, in cleanData\n",
      "    vectorizerMethod(documents_clean)\n",
      "  File \"C:\\Users\\babap\\AppData\\Local\\Temp\\ipykernel_21520\\1786529272.py\", line 4, in vectorizerMethod\n",
      "    X = vectorizer.fit_transform(documents_clean)\n",
      "  File \"C:\\Users\\babap\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2077, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\babap\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\babap\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1220, in _count_vocab\n",
      "    raise ValueError(\n",
      "ValueError: empty vocabulary; perhaps the documents only contain stop words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=0\n",
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=10\n",
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=20\n",
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=30\n",
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=40\n",
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=50\n",
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=60\n",
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=70\n",
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=80\n",
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=90\n",
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=100\n",
      "https://scholar.google.com/scholar?q=iot%2C+springer&hl=en&start=110\n",
      "12\n",
      "Started...\n",
      "Completed...\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Similarities: nan\n",
      "Link to the article: https://link.springer.com/article/10.1007/s12599-015-0383-3\n",
      "Similarities: 0.29367639321660066\n",
      "Link to the article: https://link.springer.com/article/10.1186/s42400-021-00077-7\n",
      "Similarities: 0.27627204131283073\n",
      "Link to the article: https://link.springer.com/article/10.1007/s00521-020-04874-y\n",
      "Similarities: 0.24582158150302022\n",
      "Link to the article: https://link.springer.com/article/10.1007/s11036-018-1089-9\n",
      "Similarities: 0.23104639414525263\n",
      "Link to the article: https://link.springer.com/article/10.1007/s11831-020-09496-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\babap\\AppData\\Local\\Temp\\ipykernel_21520\\3090085845.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim[i] = np.dot(df.loc[:, i].values, q_vector) / np.linalg.norm(df.loc[:,i]) * np.linalg.norm(q_vector)\n"
     ]
    }
   ],
   "source": [
    "lbl=Label(window, text=\"Enter keywords to search documents\", fg='black', font=(\"Helvetica\", 12))\n",
    "lbl.place(relx=.5, rely=.25,anchor= CENTER)\n",
    "\n",
    "\n",
    "txtfld=Entry(window, text=\"Enter keywords (',' separated)\", bd=2)\n",
    "txtfld.place(relx=.5, rely=.35,anchor= CENTER)\n",
    "\n",
    "btn=Button(window, text=\"Search Documents\", fg='black', command=lambda: collectGoogleScholarPages(txtfld.get()))\n",
    "btn.place(relx=.5, rely=.45,anchor= CENTER)\n",
    "\n",
    "window.title('Sparkler for Documents')\n",
    "window.geometry(\"500x300+250+250\")\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe325ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
