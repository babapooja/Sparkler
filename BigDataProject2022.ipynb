{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8efb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, json, os\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import gensim\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.52\",\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"q\": \"\",\n",
    "    \"hl\": \"en\",\n",
    "    \"start\": 0\n",
    "}\n",
    "from tkinter import *\n",
    "window=Tk()\n",
    "\n",
    "directory_path = '/Users/avinashgoli/Documents/BigDataProgramming/BigDataProject/DocumentDb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eaf8865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/avinashgoli/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/11/24 11:18:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/11/24 11:18:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#SPARK RELATED CODE\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Sparkler') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.caseSensitive', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc4057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensimImplementation(documents):\n",
    "    q = params['q']\n",
    "    for document in documents:\n",
    "        print(gensim.utils.simple_preprocess(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d047bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar Articles\n",
    "def getSimilarArticles(df, vectorizer, link): \n",
    "    print('-------------------getsimilarArticles----------------------------')\n",
    "    q = [params['q']]\n",
    "    q_vector = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "    print(q_vector)\n",
    "    sim = {}\n",
    "    \n",
    "    for i in range(df.shape[1]):\n",
    "        sim[i] = np.dot(df.loc[:, i].values, q_vector) / np.linalg.norm(df.loc[:,i]) * np.linalg.norm(q_vector)\n",
    "        \n",
    "    sim_sorted = sorted(sim.items(), key = lambda x:x[1], reverse = True)\n",
    "    print(sim_sorted)\n",
    "    for k,v in sim_sorted[:5]:\n",
    "        if v != 0.0:\n",
    "            print(\"Similarities: {}\".format(v))\n",
    "            print('Link to the article:', link[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac92174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizerMethod(documents_clean):\n",
    "    # Instantiate a TfIdfVectorizer Object and transform the data to vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(documents_clean)\n",
    "    X = X.T.toarray()\n",
    "\n",
    "    df = pd.DataFrame(X, index=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    print('--------------------vectorizermethod-----------------------')\n",
    "    print(df.head(2))\n",
    "    return(df, vectorizer)\n",
    "    \n",
    "    #getSimilarArticles(df, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8006de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(documents):\n",
    "    documents_clean = []\n",
    "    for d in documents:\n",
    "        # Remove Unicode\n",
    "        document_test = re.sub(r'[^\\x00-\\x7F]+', ' ', d)\n",
    "        # Remove Mentions\n",
    "        document_test = re.sub(r'@\\w+', '', document_test)\n",
    "        # Lowercase the document\n",
    "        document_test = document_test.lower()\n",
    "        # Remove punctuations\n",
    "        document_test = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', document_test)\n",
    "        # Lowercase the numbers\n",
    "        document_test = re.sub(r'[0-9]', '', document_test)\n",
    "        # Remove the doubled space\n",
    "        document_test = re.sub(r'\\s{2,}', ' ', document_test)\n",
    "        documents_clean.append(document_test)\n",
    "    print('-------------------cleandata-------------------')\n",
    "    print(documents_clean[:2])\n",
    "    return(documents_clean)\n",
    "    #vectorizerMethod(documents_clean)\n",
    "#     gensimImplementation(documents=documents_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d03f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectDocumentFromLinks(link):\n",
    "    #Retrieve paragrahs from each link, combine each paragrah as a string and save it to docs\n",
    "    documents = []\n",
    "    for i in link:\n",
    "        r = requests.get(i, headers=headers)\n",
    "        soup = BeautifulSoup(r.content,'html.parser')\n",
    "\n",
    "        sen = []\n",
    "        # for springer abstracts\n",
    "        if soup.find('div', {'id':'Abs1-content'}):\n",
    "            for i in soup.find('div', {'id':'Abs1-content'}).find_all('p'):\n",
    "                sen.append(i.text)\n",
    "\n",
    "        documents.append(' '.join(sen))\n",
    "    print('Fetched data from each link...')\n",
    "    print('------------collectDocumentfromlinks-------------------')\n",
    "    print(documents[:2])\n",
    "    return(documents)\n",
    "    #cleanData(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1bdbada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectSpringerLinks(soup):\n",
    "    #Retrieve all popular new links\n",
    "    link = []\n",
    "    print(len(soup))\n",
    "    i=0\n",
    "    for i in range(0, len(soup)):\n",
    "        data = soup[i].find_all(\"div\", {\"class\": \"gs_ri\"})\n",
    "        for j in range(len(data)):    \n",
    "\n",
    "            temp = data[j].find('a')\n",
    "\n",
    "            if 'link.springer.com/article' in temp['href'] and 'books.google.com' not in temp['href']:\n",
    "                link.append(temp['href'])\n",
    "    #             print(temp['href'], j)\n",
    "\n",
    "    #print(len(link))\n",
    "    print('---------------collectSpringerlinks-------------------')\n",
    "    print(link[:2])\n",
    "    if len(link)>0:\n",
    "        #collectDocumentFromLinks(link)\n",
    "        return(link)\n",
    "    else: \n",
    "        print('No links found. Try with other keywords.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c790b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectGoogleScholarPages(query):\n",
    "    params['q'] = query\n",
    "    soup = []\n",
    "    while True:\n",
    "        url = 'https://scholar.google.com/scholar'\n",
    "        req = requests.get(url, headers=headers, params=params)\n",
    "        print(req.url)\n",
    "        tempData = BeautifulSoup(req.content,'html.parser')\n",
    "#         print(tempData)\n",
    "        soup.append(tempData)\n",
    "        if tempData.find('span', {'class': 'gs_ico gs_ico_nav_next'}) and params['start']<=100:\n",
    "            params['start']+=10\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    print('-----------collectGoogleScholarPages-------------')\n",
    "    #print(soup)\n",
    "    # call collectSpringerLinks()\n",
    "    return soup\n",
    "    #collectSpringerLinks(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1229731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Read PDFs(Papers) from Document Database\n",
    "def readdocumentdb(directory_path):\n",
    "    DE = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename[-4:] == '.pdf':\n",
    "            filepath = directory_path+'/'+filename\n",
    "            pdfFileObj = open(filepath, 'rb')\n",
    "            pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "            numofpages = pdfReader.numPages\n",
    "            pageObj = pdfReader.getPage(0)\n",
    "            text = pageObj.extractText()\n",
    "\n",
    "            try:\n",
    "                abstractindex = re.search(r'Abstract', text, re.IGNORECASE)\n",
    "                introindex = re.search(r'(I. )?(1. )?I*ntroduction', text, re.IGNORECASE)\n",
    "                abstractend = 0\n",
    "                introstart = 0\n",
    "\n",
    "                if abstractindex:\n",
    "                    abstractend = abstractindex.end()\n",
    "                if introindex:\n",
    "                    introstart = introindex.start()\n",
    "\n",
    "                if introstart == 0:\n",
    "                    paperAbstract = text[abstractend:]\n",
    "                else:\n",
    "                    paperAbstract = text[abstractend:introstart]\n",
    "            except:\n",
    "                paperAbstract = text\n",
    "\n",
    "            paperAbstract = re.sub(r\"^\\W+\", \"\", paperAbstract.strip())\n",
    "            pdfFileObj.close()\n",
    "            A = [filepath, paperAbstract]\n",
    "            DE.append(A)\n",
    "\n",
    "    DocumentsExtract = pd.DataFrame(DE)\n",
    "    DocumentsExtract.columns = ['DocLink', 'Abstract']\n",
    "    #print(DocumentsExtract)\n",
    "    return(DocumentsExtract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34060bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter keywords on which you want to search documents:Lung Oncology\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=0\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=10\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=20\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=30\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=40\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=50\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=60\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=70\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=80\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=90\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=100\n",
      "https://scholar.google.com/scholar?q=Lung+Oncology&hl=en&start=110\n",
      "-----------collectGoogleScholarPages-------------\n",
      "12\n",
      "---------------collectSpringerlinks-------------------\n",
      "['https://link.springer.com/article/10.1007/s11748-010-0743-3', 'https://link.springer.com/article/10.1007/s12094-019-02218-4']\n",
      "Fetching data from each link...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched data from each link...\n",
      "------------collectDocumentfromlinks-------------------\n",
      "['Progress in genetic engineering has made it possible to elucidate the molecular biological abnormalities in lung cancer. Mutations in KRAS and P53 genes, loss of specific alleles, and DNA methylation of the tumor suppressor genes were the major abnormalities investigated between 1980 and the 2000s. In 2004, mutations in the epidermal growth factor receptor (EGFR) gene that cause oncogene addiction were discovered in non-small-cell lung cancers (NSCLCs), especially in adenocarcinomas. Because they are strongly associated with sensitivity to EGFR-tyrosine kinase inhibitors (EGFR-TKIs), a great deal of knowledge has been acquired in regard to both EGFR and other genes in the EGFR family and their downstream genes. Moreover, in 2007 the existence of the echinoderm microtubule-associated protein-like 4 (EML4)-anaplastic lymphoma kinase (ALK) fusion gene was discovered in NSCLC; and the same as EGFR-TKIs, ALK inhibitors are being found to be highly effective in lung cancers that have this translocation. These discoveries graphically illustrate that molecular biological findings are directly linked to the development of clinical oncology and to improving the survival rates of lung cancer patients. Here, we review the remarkable progress in molecular biological knowledge acquired thus far in regard to lung cancer, especially NSCLC, and the future possibilities.', 'In 2011 the Spanish Society of Medical Oncology (SEOM) and the Spanish Society of Pathology (SEAP) started a joint project to establish guidelines on biomarker testing in patients with advanced non-small-cell lung cancer (NSCLC) based on current evidence. As this field is constantly evolving, these guidelines have been updated, previously in 2012 and 2015 and now in 2019. Current evidence suggests that the mandatory tests to conduct in all patients with advanced NSCLC are for EGFR and BRAF mutations, ALK and ROS1 rearrangements and PD-L1 expression. The growing need to study other emerging biomarkers has promoted the routine use of massive sequencing (next-generation sequencing, NGS). The coordination of every professional involved and the prioritisation of the most suitable tests and technologies for each case remains a challenge.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------cleandata-------------------\n",
      "['progress in genetic engineering has made it possible to elucidate the molecular biological abnormalities in lung cancer mutations in kras and p genes loss of specific alleles and dna methylation of the tumor suppressor genes were the major abnormalities investigated between and the s in mutations in the epidermal growth factor receptor egfr gene that cause oncogene addiction were discovered in non small cell lung cancers nsclcs especially in adenocarcinomas because they are strongly associated with sensitivity to egfr tyrosine kinase inhibitors egfr tkis a great deal of knowledge has been acquired in regard to both egfr and other genes in the egfr family and their downstream genes moreover in the existence of the echinoderm microtubule associated protein like eml anaplastic lymphoma kinase alk fusion gene was discovered in nsclc and the same as egfr tkis alk inhibitors are being found to be highly effective in lung cancers that have this translocation these discoveries graphically illustrate that molecular biological findings are directly linked to the development of clinical oncology and to improving the survival rates of lung cancer patients here we review the remarkable progress in molecular biological knowledge acquired thus far in regard to lung cancer especially nsclc and the future possibilities ', 'in the spanish society of medical oncology seom and the spanish society of pathology seap started a joint project to establish guidelines on biomarker testing in patients with advanced non small cell lung cancer nsclc based on current evidence as this field is constantly evolving these guidelines have been updated previously in and and now in current evidence suggests that the mandatory tests to conduct in all patients with advanced nsclc are for egfr and braf mutations alk and ros rearrangements and pd l expression the growing need to study other emerging biomarkers has promoted the routine use of massive sequencing next generation sequencing ngs the coordination of every professional involved and the prioritisation of the most suitable tests and technologies for each case remains a challenge ']\n",
      "--------------------vectorizermethod-----------------------\n",
      "           0    1    2    3    4    5    6    7    8    9   ...   29   30  \\\n",
      "aamir     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "abdullah  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "\n",
      "           31   32   33   34   35   36   37   38  \n",
      "aamir     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "abdullah  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[2 rows x 39 columns]\n",
      "-------------------getsimilarArticles----------------------------\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[(0, 0.2348914232359521), (7, 0.17541260493509067), (12, 0.11722086524510503), (1, 0.10686015372431915), (5, 0.07762096264632516), (15, 0.07762096264632516), (10, 0.05542507794809963), (36, 0.04519003376788317), (2, 0.032729308211113584), (3, 0.0), (4, 0.0), (6, 0.0), (8, 0.0), (9, 0.0), (11, 0.0), (13, 0.0), (14, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (37, 0.0), (38, 0.0)]\n",
      "Similarities: 0.2348914232359521\n",
      "Link to the article: https://link.springer.com/article/10.1007/s11748-010-0743-3\n",
      "Similarities: 0.17541260493509067\n",
      "Link to the article: /Users/avinashgoli/Documents/BigDataProgramming/BigDataProject/DocumentDb/Oncology-7.pdf\n",
      "Similarities: 0.11722086524510503\n",
      "Link to the article: /Users/avinashgoli/Documents/BigDataProgramming/BigDataProject/DocumentDb/Oncology-2.pdf\n",
      "Similarities: 0.10686015372431915\n",
      "Link to the article: https://link.springer.com/article/10.1007/s12094-019-02218-4\n",
      "Similarities: 0.07762096264632516\n",
      "Link to the article: /Users/avinashgoli/Documents/BigDataProgramming/BigDataProject/DocumentDb/Oncology-5.pdf\n"
     ]
    }
   ],
   "source": [
    "# lbl=Label(window, text=\"Enter keywords to search documents\", fg='black', font=(\"Helvetica\", 12))\n",
    "# lbl.place(relx=.5, rely=.25,anchor= CENTER)\n",
    "\n",
    "# txtfld=Entry(window, text=\"Enter keywords (',' separated)\", bd=2)\n",
    "# txtfld.place(relx=.5, rely=.35,anchor= CENTER)\n",
    "\n",
    "# btn=Button(window, text=\"Search Documents\", fg='black', command=lambda: collectGoogleScholarPages(txtfld.get()))\n",
    "# btn.place(relx=.5, rely=.45,anchor= CENTER)\n",
    "\n",
    "# window.title('Sparkler for Documents')\n",
    "# window.geometry(\"500x300+250+250\")\n",
    "# window.mainloop()\n",
    "\n",
    "keywords = input(\"Enter keywords on which you want to search documents:\")\n",
    "\n",
    "\"\"\"\n",
    "##Reading google scholar\n",
    "soup = collectGoogleScholarPages(keywords)\n",
    "links = collectSpringerLinks(soup)\n",
    "docs = collectDocumentFromLinks(links)\n",
    "cdocs = cleanData(docs)\n",
    "features, vector = vectorizerMethod(cdocs)\n",
    "getSimilarArticles(features, vector, links)\n",
    "\n",
    "\n",
    "\n",
    "dbdocs = readdocumentdb(directory_path)\n",
    "dbdocsabstract = dbdocs[\"Abstract\"].to_numpy().tolist()\n",
    "dbdocslinks = dbdocs[\"DocLink\"].to_numpy().tolist()\n",
    "cleanabstractdata = cleanData(dbdocsabstract)\n",
    "featuredocs, vectordocs = vectorizerMethod(cleanabstractdata)\n",
    "getSimilarArticles(featuredocs, vectordocs, dbdocslinks)\n",
    "\"\"\"\n",
    "\n",
    "##Reading google scholar\n",
    "soup = collectGoogleScholarPages(keywords)\n",
    "links = collectSpringerLinks(soup)\n",
    "docs = collectDocumentFromLinks(links)\n",
    "\n",
    "##read all the documents from db\n",
    "dbdocs = readdocumentdb(directory_path)\n",
    "dbdocsabstract = dbdocs[\"Abstract\"].to_numpy().tolist()\n",
    "dbdocslinks = dbdocs[\"DocLink\"].to_numpy().tolist()\n",
    "\n",
    "combinedlinks = links + dbdocslinks\n",
    "combineddocs = docs + dbdocsabstract\n",
    "\n",
    "cleandataset = cleanData(combineddocs)\n",
    "featureds, vectords = vectorizerMethod(cleandataset)\n",
    "getSimilarArticles(featureds, vectords, combinedlinks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
