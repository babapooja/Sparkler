{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8efb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED IMPORTS\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, json, os\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import gensim\n",
    "import PyPDF2\n",
    "import os, math\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eaf8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPARK RELATED CODE\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Sparkler') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.caseSensitive', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c8ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENSIM RELATED IMPORTS\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc4057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.52\",\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"q\": \"\",\n",
    "    \"hl\": \"en\",\n",
    "    \"start\": 0\n",
    "}\n",
    "\n",
    "directory_path = 'DocumentDb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4e030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2VecImplementation(cleandataset):\n",
    "    docTokensCollection = spark.sparkContext.parallelize(cleandataset).map(lambda line: line.split(\" \"))\n",
    "    word2Vec = Word2Vec().setVectorSize(10).setMinCount(6).setSeed(42)\n",
    "    model = word2Vec.fit(docTokensCollection)\n",
    "    for i in params['q'].replace(',',' ').split(\" \"):\n",
    "        try:\n",
    "            res = model.findSynonyms(i, 3)\n",
    "            print('Words similar to {} (with respect to other words) are as follows - '.format(i))\n",
    "            print(tabulate(res, headers=[\"Word\", \"Similarity\"], tablefmt='grid'))    \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e4860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the file and save in DocumentDb folder\n",
    "def downloadFile(link):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d047bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar Articles\n",
    "def getSimilarArticles(df, vectorizer, link): \n",
    "    print('------------- TF-IDF FOR CALCULATING THE SIMILAR DOCUMENTS FOR THE KEYWORDS -------------')\n",
    "    q = [params['q']]\n",
    "    q_vector = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "    sim = {}\n",
    "\n",
    "    for i in range(df.shape[1]):\n",
    "        sim[i] = np.dot(df.loc[:, i].values, q_vector) / np.linalg.norm(df.loc[:,i]) * np.linalg.norm(q_vector)\n",
    "        \n",
    "    sim_sorted = sorted(sim.items(), key = lambda x:x[1], reverse = True)\n",
    "    print('Top 5 documents that are most relevant to the keywords entered are -')\n",
    "    for k,v in sim_sorted[:5]:\n",
    "        if v != 0.0 and math.isnan(v)==False:\n",
    "            print(\"Similarities: {}\".format(v))\n",
    "            print('Link to the article:', link[k])\n",
    "            if('https' in link[k]):\n",
    "                downloadFile(link[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac92174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizerMethod(documents_clean):\n",
    "    # Instantiate a TfIdfVectorizer Object and transform the data to vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(documents_clean)\n",
    "    X = X.T.toarray()\n",
    "    df = pd.DataFrame(X, index=vectorizer.get_feature_names_out())    \n",
    "    return(df, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8006de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(documents):\n",
    "    print('------------- DATA PREPROCESSING -------------')\n",
    "    print('Cleaning data for removing any unicodes, mentions, punctuations, double spaces, stopwords.\\nConverting the data to lower case.')\n",
    "    documents_clean = []\n",
    "    for d in documents:\n",
    "        # Remove Unicode\n",
    "        document_test = re.sub(r'[^\\x00-\\x7F]+', ' ', d)\n",
    "        # Remove Mentions\n",
    "        document_test = re.sub(r'@\\w+', '', document_test)\n",
    "        # Lowercase the document\n",
    "        document_test = document_test.lower()\n",
    "        # Remove punctuations\n",
    "        document_test = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', document_test)\n",
    "        # Lowercase the numbers\n",
    "        document_test = re.sub(r'[0-9]', '', document_test)\n",
    "        # Remove the doubled space\n",
    "        document_test = re.sub(r'\\s{2,}', ' ', document_test)\n",
    "        documents_clean.append(remove_stopwords(document_test))\n",
    "    print('Cleaned the data.')\n",
    "    return(documents_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d03f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectDocumentFromLinks(link):\n",
    "    print('------------- COLLECTING ABSTRACT DATA FOR EACH SPRINGER LINK -------------')\n",
    "    #Retrieve paragrahs from each link, combine each paragrah as a string and save it to docs\n",
    "    documents = []\n",
    "    for i in link:\n",
    "        r = requests.get(i, headers=headers)\n",
    "        soup = BeautifulSoup(r.content,'html.parser')\n",
    "\n",
    "        sen = []\n",
    "        # for springer abstracts\n",
    "        if soup.find('div', {'id':'Abs1-content'}):\n",
    "            for i in soup.find('div', {'id':'Abs1-content'}).find_all('p'):\n",
    "                sen.append(i.text)\n",
    "        documents.append(' '.join(sen))\n",
    "    print('Collected abstract data from {} springer links fetched earlier'.format(len(documents)))\n",
    "    return(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1bdbada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectSpringerLinks(soup):\n",
    "    print('------------- COLLECTING RELEVANT SPRINGER LINKS FOR THE QUERY -------------')\n",
    "    #Retrieve all popular new links\n",
    "    link = []\n",
    "    i=0\n",
    "    for i in range(0, len(soup)):\n",
    "        data = soup[i].find_all(\"div\", {\"class\": \"gs_ri\"})\n",
    "        for j in range(len(data)):    \n",
    "            temp = data[j].find('a')\n",
    "            if 'link.springer.com/article' in temp['href'] and 'books.google.com' not in temp['href']:\n",
    "                link.append(temp['href'])\n",
    "    if len(link)>0:\n",
    "        print('Collected {} links'.format(len(link)))\n",
    "        return(link)\n",
    "    else: \n",
    "        print('No springer links found. Try with other keywords.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c790b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectGoogleScholarPages(query):\n",
    "    print('------------- VISITING GOOGLE SCHOLAR PAGES TO FETCH URLS -------------')\n",
    "    params['q'] = query\n",
    "    soup = []\n",
    "    while True:\n",
    "        url = 'https://scholar.google.com/scholar'\n",
    "        req = requests.get(url, headers=headers, params=params)\n",
    "        print(req.url)\n",
    "        tempData = BeautifulSoup(req.content,'html.parser')\n",
    "        soup.append(tempData)\n",
    "        if tempData.find('span', {'class': 'gs_ico gs_ico_nav_next'}) and params['start']<100:\n",
    "            params['start']+=10\n",
    "        else:\n",
    "            break\n",
    "    print('Visited {} google scholar pages'.format(int(params['start'])/10))\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1229731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Read PDFs(Papers) from Document Database\n",
    "def readdocumentdb():\n",
    "    DE = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename[-4:] == '.pdf':\n",
    "            filepath = directory_path+'/'+filename\n",
    "            pdfFileObj = open(filepath, 'rb')\n",
    "            pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "            numofpages = pdfReader.numPages\n",
    "            pageObj = pdfReader.getPage(0)\n",
    "            text = pageObj.extractText()\n",
    "\n",
    "            try:\n",
    "                abstractindex = re.search(r'Abstract', text, re.IGNORECASE)\n",
    "                introindex = re.search(r'(I. )?(1. )?I*ntroduction', text, re.IGNORECASE)\n",
    "                abstractend = 0\n",
    "                introstart = 0\n",
    "\n",
    "                if abstractindex:\n",
    "                    abstractend = abstractindex.end()\n",
    "                if introindex:\n",
    "                    introstart = introindex.start()\n",
    "\n",
    "                if introstart == 0:\n",
    "                    paperAbstract = text[abstractend:]\n",
    "                else:\n",
    "                    paperAbstract = text[abstractend:introstart]\n",
    "            except:\n",
    "                paperAbstract = text\n",
    "\n",
    "            paperAbstract = re.sub(r\"^\\W+\", \"\", paperAbstract.strip())\n",
    "            pdfFileObj.close()\n",
    "            A = [filepath, paperAbstract]\n",
    "            DE.append(A)\n",
    "\n",
    "    DocumentsExtract = pd.DataFrame(DE)\n",
    "    DocumentsExtract.columns = ['DocLink', 'Abstract']\n",
    "    return(DocumentsExtract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34060bd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter keywords on which you want to search documents:brain tumor cancer oncology checmotherapy\n",
      "------------- VISITING GOOGLE SCHOLAR PAGES TO FETCH URLS -------------\n",
      "https://scholar.google.com/scholar?q=brain+tumor+cancer+oncology+checmotherapy&hl=en&start=0\n",
      "https://scholar.google.com/scholar?q=brain+tumor+cancer+oncology+checmotherapy&hl=en&start=10\n",
      "https://scholar.google.com/scholar?q=brain+tumor+cancer+oncology+checmotherapy&hl=en&start=20\n",
      "https://scholar.google.com/scholar?q=brain+tumor+cancer+oncology+checmotherapy&hl=en&start=30\n",
      "https://scholar.google.com/scholar?q=brain+tumor+cancer+oncology+checmotherapy&hl=en&start=40\n",
      "https://scholar.google.com/scholar?q=brain+tumor+cancer+oncology+checmotherapy&hl=en&start=50\n",
      "Visited 5.0 google scholar pages\n",
      "------------- COLLECTING RELEVANT SPRINGER LINKS FOR THE QUERY -------------\n",
      "Collected 3 links\n",
      "------------- COLLECTING ABSTRACT DATA FOR EACH SPRINGER LINK -------------\n",
      "Collected abstract data from 3 springer links fetched earlier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "C:\\Users\\babap\\AppData\\Local\\Temp\\ipykernel_5984\\2776739302.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim[i] = np.dot(df.loc[:, i].values, q_vector) / np.linalg.norm(df.loc[:,i]) * np.linalg.norm(q_vector)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- DATA PREPROCESSING -------------\n",
      "Cleaning data for removing any unicodes, mentions, punctuations, double spaces, stopwords.\n",
      "Converting the data to lower case.\n",
      "Cleaned the data.\n",
      "------------- TF-IDF FOR CALCULATING THE SIMILAR DOCUMENTS FOR THE KEYWORDS -------------\n",
      "Top 5 documents that are most relevant to the keywords entered are -\n",
      "Similarities: 0.34339633388628565\n",
      "Link to the article: DocumentDb/BrainTumor-2.pdf\n",
      "Similarities: 0.34339633388628565\n",
      "Link to the article: DocumentDb/BrainTumor-6.pdf\n",
      "Similarities: 0.2404778107666957\n",
      "Link to the article: DocumentDb/BrainTumor-5.pdf\n",
      "Similarities: 0.23286722811238117\n",
      "Link to the article: DocumentDb/Oncology-1.pdf\n",
      "Words similar to brain (with respect to other words) are as follows - \n",
      "+--------------+--------------+\n",
      "| Word         |   Similarity |\n",
      "+==============+==============+\n",
      "| tumor        |     0.960456 |\n",
      "+--------------+--------------+\n",
      "| image        |     0.955047 |\n",
      "+--------------+--------------+\n",
      "| segmentation |     0.947331 |\n",
      "+--------------+--------------+\n",
      "Words similar to tumor (with respect to other words) are as follows - \n",
      "+--------+--------------+\n",
      "| Word   |   Similarity |\n",
      "+========+==============+\n",
      "| brain  |     0.960456 |\n",
      "+--------+--------------+\n",
      "| images |     0.92931  |\n",
      "+--------+--------------+\n",
      "| mri    |     0.927413 |\n",
      "+--------+--------------+\n",
      "Words similar to cancer (with respect to other words) are as follows - \n",
      "+---------+--------------+\n",
      "| Word    |   Similarity |\n",
      "+=========+==============+\n",
      "| user    |     0.919813 |\n",
      "+---------+--------------+\n",
      "| gr      |     0.911048 |\n",
      "+---------+--------------+\n",
      "| publica |     0.897258 |\n",
      "+---------+--------------+\n",
      "Words similar to oncology (with respect to other words) are as follows - \n",
      "+--------------+--------------+\n",
      "| Word         |   Similarity |\n",
      "+==============+==============+\n",
      "| technologies |     0.853533 |\n",
      "+--------------+--------------+\n",
      "| research     |     0.835876 |\n",
      "+--------------+--------------+\n",
      "| data         |     0.798196 |\n",
      "+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# ##Reading google scholar\n",
    "# soup = collectGoogleScholarPages(keywords)\n",
    "# links = collectSpringerLinks(soup)\n",
    "# docs = collectDocumentFromLinks(links)\n",
    "# cdocs = cleanData(docs)\n",
    "# features, vector = vectorizerMethod(cdocs)\n",
    "# getSimilarArticles(features, vector, links)\n",
    "\n",
    "\n",
    "\n",
    "# dbdocs = readdocumentdb(directory_path)\n",
    "# dbdocsabstract = dbdocs[\"Abstract\"].to_numpy().tolist()\n",
    "# dbdocslinks = dbdocs[\"DocLink\"].to_numpy().tolist()\n",
    "# cleanabstractdata = cleanData(dbdocsabstract)\n",
    "# featuredocs, vectordocs = vectorizerMethod(cleanabstractdata)\n",
    "# getSimilarArticles(featuredocs, vectordocs, dbdocslinks)\n",
    "\n",
    "# Entry point of the code\n",
    "\n",
    "keywords = input(\"Enter keywords on which you want to search documents:\")\n",
    "\n",
    "\n",
    "##Reading google scholar\n",
    "soup = collectGoogleScholarPages(keywords)\n",
    "links = collectSpringerLinks(soup)\n",
    "docs = collectDocumentFromLinks(links)\n",
    "\n",
    "##read all the documents from db\n",
    "dbdocs = readdocumentdb()\n",
    "dbdocsabstract = dbdocs[\"Abstract\"].to_numpy().tolist()\n",
    "dbdocslinks = dbdocs[\"DocLink\"].to_numpy().tolist()\n",
    "\n",
    "combinedlinks = links + dbdocslinks\n",
    "combineddocs = docs + dbdocsabstract\n",
    "\n",
    "cleandataset = cleanData(combineddocs)\n",
    "featureds, vectords = vectorizerMethod(cleandataset)\n",
    "getSimilarArticles(featureds, vectords, combinedlinks)\n",
    "word2VecImplementation(cleandataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0bf6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
